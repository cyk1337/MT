Japanese English dataset configuration
vocab size, en=3713, fr=3949
--------------------------------------------------
Training progress will be logged in:
	model/train_10000sen_2-3layers_100units_ja_en_exp1_SOFT_ATTN_dropout_0.00.log
--------------------------------------------------
Trained model will be saved as:
	model/seq2seq_10000sen_2-3layers_100units_ja_en_exp1_SOFT_ATTN_dropout_0.00.model
--------------------------------------------------
Existing model not found!
--------------------------------------------------
--------------------------------------------------
precision  | 0.1893
recall     | 0.1839
f1         | 0.1866
--------------------------------------------------
computing perplexity
--------------------------------------------------
dev perplexity | 48.8440
# words in dev | 4557
--------------------------------------------------
Saving model
Finished saving model
--------------------------------------------------
computing bleu
BLEU: 4.562
finished computing bleu ... 
--------------------------------------------------
--------------------------------------------------
precision  | 0.2225
recall     | 0.2179
f1         | 0.2202
--------------------------------------------------
computing perplexity
--------------------------------------------------
dev perplexity | 42.9871
# words in dev | 4557
--------------------------------------------------
Saving model
Finished saving model
--------------------------------------------------
--------------------------------------------------
precision  | 0.2281
recall     | 0.2243
f1         | 0.2262
--------------------------------------------------
computing perplexity
--------------------------------------------------
dev perplexity | 40.2159
# words in dev | 4557
--------------------------------------------------
Saving model
Finished saving model
--------------------------------------------------
computing bleu
BLEU: 7.797
finished computing bleu ... 
--------------------------------------------------
--------------------------------------------------
precision  | 0.2295
recall     | 0.2276
f1         | 0.2285
--------------------------------------------------
computing perplexity
--------------------------------------------------
dev perplexity | 38.3625
# words in dev | 4557
--------------------------------------------------
Saving model
Finished saving model
--------------------------------------------------
--------------------------------------------------
precision  | 0.2288
recall     | 0.2298
f1         | 0.2293
--------------------------------------------------
computing perplexity
--------------------------------------------------
dev perplexity | 36.9376
# words in dev | 4557
--------------------------------------------------
Saving model
Finished saving model
--------------------------------------------------
computing bleu
BLEU: 9.175
finished computing bleu ... 
--------------------------------------------------
--------------------------------------------------
precision  | 0.2339
recall     | 0.2374
f1         | 0.2357
--------------------------------------------------
computing perplexity
--------------------------------------------------
dev perplexity | 35.8361
# words in dev | 4557
--------------------------------------------------
Saving model
Finished saving model
--------------------------------------------------
--------------------------------------------------
precision  | 0.2369
recall     | 0.2449
f1         | 0.2409
--------------------------------------------------
computing perplexity
--------------------------------------------------
dev perplexity | 35.0672
# words in dev | 4557
--------------------------------------------------
Saving model
Finished saving model
--------------------------------------------------
computing bleu
BLEU: 10.870
finished computing bleu ... 
--------------------------------------------------
--------------------------------------------------
precision  | 0.2441
recall     | 0.2535
f1         | 0.2487
--------------------------------------------------
computing perplexity
--------------------------------------------------
dev perplexity | 34.5414
# words in dev | 4557
--------------------------------------------------
Saving model
Finished saving model
--------------------------------------------------
--------------------------------------------------
precision  | 0.2445
recall     | 0.2548
f1         | 0.2495
--------------------------------------------------
computing perplexity
--------------------------------------------------
dev perplexity | 34.2170
# words in dev | 4557
--------------------------------------------------
Saving model
Finished saving model
--------------------------------------------------
computing bleu
BLEU: 11.928
finished computing bleu ... 
--------------------------------------------------
--------------------------------------------------
precision  | 0.2438
recall     | 0.2576
f1         | 0.2505
--------------------------------------------------
computing perplexity
--------------------------------------------------
dev perplexity | 34.1993
# words in dev | 4557
--------------------------------------------------
Saving model
Finished saving model
--------------------------------------------------
--------------------------------------------------
precision  | 0.2469
recall     | 0.2578
f1         | 0.2523
--------------------------------------------------
computing perplexity
--------------------------------------------------
dev perplexity | 34.3984
# words in dev | 4557
--------------------------------------------------
Saving model
Finished saving model
--------------------------------------------------
computing bleu
BLEU: 12.787
finished computing bleu ... 
--------------------------------------------------
--------------------------------------------------
precision  | 0.2497
recall     | 0.2627
f1         | 0.2560
--------------------------------------------------
computing perplexity
--------------------------------------------------
dev perplexity | 34.6883
# words in dev | 4557
--------------------------------------------------
Saving model
Finished saving model
--------------------------------------------------
Training set predictions
English predictions, s=10000, num=5:
--------------------------------------------------
Src | この 路地 は 通り抜け でき ま せ ん 。                                                         
Ref | this is a dead @-@ end alley .                                                  
Hyp | this can &apos;t _UNK this this . _EOS                                          
--------------------------------------------------
precision | 0.2500
recall | 0.2500
--------------------------------------------------
Src | ええ 、 届 い た の を お 知 らせ する の を 忘れ て しま っ て す み ま せ ん 。                            
Ref | yes , sorry , i forgot to acknowledge it .                                      
Hyp | i &apos;t &apos;t you you you you you you you you you . . . _EOS                
--------------------------------------------------
precision | 0.1250
recall | 0.2000
--------------------------------------------------
Src | 彼 は ドイツ 生まれ の 人 だ 。                                                             
Ref | he is a german by origin .                                                      
Hyp | he is a a a . . _EOS                                                            
--------------------------------------------------
precision | 0.5000
recall | 0.5714
--------------------------------------------------
Src | 我々 は ロビン フッド の 伝説 を 良く 知 っ て い る 。                                              
Ref | we are familiar with the legend of robin hood .                                 
Hyp | we are _UNK _UNK _UNK . . _EOS                                                  
--------------------------------------------------
precision | 0.3750
recall | 0.3000
--------------------------------------------------
Src | トランク に は 鍵 が かけ られ て い ま す か 。                                                  
Ref | is your trunk locked ?                                                          
Hyp | can is _UNK in in _UNK ? ? _EOS                                                 
--------------------------------------------------
precision | 0.2222
recall | 0.4000
sentences matching filter = 5
plot attention 
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
================================================================================
English predictions, s=0, num=3:
--------------------------------------------------
Src | ステーキ は 中位 で 焼 い て くださ い 。                                                       
Ref | i like my steak medium .                                                        
Hyp | please your in in . . _EOS                                                      
--------------------------------------------------
precision | 0.1429
recall | 0.1667
--------------------------------------------------
Src | 彼女 の 美し さ に 関 し て は 、 疑 う 余地 が な い 。                                            
Ref | there is no doubt as to her beauty .                                            
Hyp | she is not not her her her . . _EOS                                             
--------------------------------------------------
precision | 0.3000
recall | 0.3333
--------------------------------------------------
Src | この 近所 の 家 は どれ も とても よく 似 て い る の で 見分け が つ か な い 。                             
Ref | all the houses in this neighborhood look so much alike that i can &apos;t tell them apart .
Hyp | this is is this this this this this . . . . . . . _EOS                          
--------------------------------------------------
precision | 0.1250
recall | 0.1111
sentences matching filter = 3
--------------------------------------------------
dev set predictions
English predictions, s=10000, num=3:
--------------------------------------------------
Src | この 路地 は 通り抜け でき ま せ ん 。                                                         
Ref | this is a dead @-@ end alley .                                                  
Hyp | this can &apos;t _UNK this this . _EOS                                          
--------------------------------------------------
precision | 0.2500
recall | 0.2500
--------------------------------------------------
Src | ええ 、 届 い た の を お 知 らせ する の を 忘れ て しま っ て す み ま せ ん 。                            
Ref | yes , sorry , i forgot to acknowledge it .                                      
Hyp | i &apos;t &apos;t you you you you you you you you you . . . _EOS                
--------------------------------------------------
precision | 0.1250
recall | 0.2000
--------------------------------------------------
Src | 彼 は ドイツ 生まれ の 人 だ 。                                                             
Ref | he is a german by origin .                                                      
Hyp | he is a a a . . _EOS                                                            
--------------------------------------------------
precision | 0.5000
recall | 0.5714
sentences matching filter = 3
--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
Finished training. Filenames:
model/train_10000sen_2-3layers_100units_ja_en_exp1_SOFT_ATTN_dropout_0.00.log
model/seq2seq_10000sen_2-3layers_100units_ja_en_exp1_SOFT_ATTN_dropout_0.00.model



epoch=1, iter=4200, loss=5.594202, mean loss=6.228269:  42%|██████████████████████▋                               | 4200/10000 [12:55<17:40,  5.47it/s]epoch=1, iter=10000, loss=6.271989, mean loss=6.019831: 100%|████████████████████████████████████████████████████| 10000/10000 [30:30<00:00,  5.56it/s]
loss=5.546427: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [00:23<00:00, 23.70it/s]
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [00:19<00:00, 29.71it/s]
epoch=2, iter=20000, loss=6.090590, mean loss=5.673634: 100%|████████████████████████████████████████████████████| 10000/10000 [29:49<00:00,  5.83it/s]
loss=5.679835: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [00:22<00:00, 22.14it/s]
epoch=3, iter=30000, loss=5.925268, mean loss=5.523909: 100%|████████████████████████████████████████████████████| 10000/10000 [29:41<00:00,  5.75it/s]
loss=5.723114: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [00:22<00:00, 22.40it/s]
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [00:19<00:00, 25.92it/s]
epoch=4, iter=40000, loss=5.811067, mean loss=5.421104: 100%|████████████████████████████████████████████████████| 10000/10000 [29:46<00:00,  5.63it/s]
loss=5.703946: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [00:22<00:00, 22.16it/s]
epoch=5, iter=50000, loss=5.731656, mean loss=5.331119: 100%|████████████████████████████████████████████████████| 10000/10000 [29:41<00:00,  5.70it/s]
loss=5.696369: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [00:22<00:00, 22.21it/s]
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [00:19<00:00, 25.23it/s]
epoch=6, iter=60000, loss=5.711772, mean loss=5.246799: 100%|████████████████████████████████████████████████████| 10000/10000 [29:54<00:00,  5.63it/s]
loss=5.802592: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [00:22<00:00, 21.90it/s]
epoch=7, iter=70000, loss=5.672572, mean loss=5.168474: 100%|████████████████████████████████████████████████████| 10000/10000 [29:47<00:00,  5.81it/s]
loss=6.078478: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [00:22<00:00, 22.14it/s]
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [00:19<00:00, 25.76it/s]
epoch=8, iter=80000, loss=5.647192, mean loss=5.095086: 100%|████████████████████████████████████████████████████| 10000/10000 [29:44<00:00,  5.61it/s]
loss=6.079144: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [00:22<00:00, 22.04it/s]
epoch=9, iter=90000, loss=5.596604, mean loss=5.026088: 100%|████████████████████████████████████████████████████| 10000/10000 [29:42<00:00,  5.77it/s]
loss=6.044519: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [00:22<00:00, 23.96it/s]
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [00:19<00:00, 25.64it/s]
epoch=10, iter=100000, loss=5.567653, mean loss=4.959744: 100%|██████████████████████████████████████████████████| 10000/10000 [29:44<00:00,  5.79it/s]
loss=6.086985: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [00:22<00:00, 21.96it/s]
epoch=11, iter=110000, loss=5.461673, mean loss=4.896548: 100%|██████████████████████████████████████████████████| 10000/10000 [29:47<00:00,  5.72it/s]
loss=5.896818: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [00:22<00:00, 21.95it/s]
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [00:19<00:00, 25.87it/s]
epoch=12, iter=120000, loss=5.392244, mean loss=4.835417: 100%|██████████████████████████████████████████████████| 10000/10000 [29:48<00:00,  5.69it/s]
loss=5.614531: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [00:22<00:00, 22.21it/s]
